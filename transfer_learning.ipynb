{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.0 64-bit ('3dcv': conda)",
      "metadata": {
        "interpreter": {
          "hash": "5bbec2c175cdc645bdcaa7c23b1994df206e8fec5e7a0d34dbfaad9347bbf153"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcvOIptdiZR4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pathlib"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxNW3fARio1b"
      },
      "source": [
        "# Load feature vector extractor into KerasLayer\n",
        "# r50x1_loc = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
        "r50x1_loc = \"models/bit_m-r50x1_1\"\n",
        "feat_vec_layer = hub.KerasLayer(r50x1_loc, name='feat_vec_embedding')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oblh_pZJjrWN"
      },
      "source": [
        "def preprocess_image(image):\n",
        "  image = np.array(image)\n",
        "  # reshape into shape [batch_size, height, width, num_channels]\n",
        "  img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)  \n",
        "  return image\n",
        "\n",
        "def load_image_from_url(url):\n",
        "  \"\"\"Returns an image with shape [1, height, width, num_channels].\"\"\"\n",
        "  response = requests.get(url)\n",
        "  image = Image.open(BytesIO(response.content))\n",
        "  image = preprocess_image(image)\n",
        "  return image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nq2qF1Ci0wc"
      },
      "source": [
        "# test feature vector\n",
        "\n",
        "# Load image\n",
        "img_url = \"https://p0.pikrepo.com/preview/853/907/close-up-photo-of-gray-elephant.jpg\"\n",
        "image = load_image_from_url(img_url)\n",
        "\n",
        "# Run model on image\n",
        "logits = feat_vec_layer(image)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsRxPuCFjxOe",
        "outputId": "44044ca0-4f05-4932-8bd4-c4299e3cb944"
      },
      "source": [
        "logits # 2048-dimensional feature vector"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2048), dtype=float32, numpy=\n",
              "array([[0.31866187, 0.        , 8.563894  , ..., 0.79412353, 0.53980786,\n",
              "        6.8815365 ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = tf.keras.Sequential([feat_vec_layer, tf.keras.layers.Dense(2, activation='softmax', kernel_initializer='zeros')])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTG-27noj1h1"
      },
      "source": [
        "# class RGB_model(tf.keras.Model):\n",
        "#   \"\"\"transfer learning model using feature vector and custom new head\"\"\"\n",
        "\n",
        "#   def __init__(self, num_classes, embedding_layer):\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.num_classes = num_classes\n",
        "#     self.rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "#     self.embedding_layer = embedding_layer\n",
        "#     self.predict = tf.keras.layers.Dense(num_classes)\n",
        "  \n",
        "#   def call(self, images):\n",
        "#     rescaled = self.rescale(images)\n",
        "#     img_embedding = self.embedding_layer(rescaled)\n",
        "#     pred = self.predict(img_embedding)\n",
        "    \n",
        "#     return pred"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "input_shape = [200, 200, 3]\n",
        "num_classes = 2\n",
        "input_image = Input(shape=input_shape, name='input_image')\n",
        "rescaled_image = Rescaling(1./255, name='normalize')(input_image)\n",
        "image_embedding = feat_vec_layer(rescaled_image)\n",
        "output = Dense(num_classes, activation='softmax', name='output')(image_embedding)\n",
        "\n",
        "\n",
        "rgb_model = tf.keras.Model(inputs=[input_image], outputs=[output])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_image (InputLayer)     [(None, 200, 200, 3)]     0         \n_________________________________________________________________\nnormalize (Rescaling)        (None, 200, 200, 3)       0         \n_________________________________________________________________\nfeat_vec_embedding (KerasLay (None, 2048)              23500352  \n_________________________________________________________________\noutput (Dense)               (None, 2)                 4098      \n=================================================================\nTotal params: 23,504,450\nTrainable params: 4,098\nNon-trainable params: 23,500,352\n_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "rgb_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(rgb_model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ln4TmyE2VmH"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "IM_SIZE = (200, 200)\n",
        "seed = np.random.randint(0, int(1e6))\n",
        "rgb_train_path = \"C:/Users/awnya/Documents/Projects/RGBD Object Classification/RGB_dataset/train\"\n",
        "rgb_dataset_train = image_dataset_from_directory(rgb_train_path, labels='inferred', label_mode='categorical', \n",
        "                                                                  image_size=IM_SIZE, batch_size=32, shuffle=True, \n",
        "                                                                  validation_split=0.05, subset='training', seed=seed)\n",
        "rgb_dataset_valid = image_dataset_from_directory(rgb_train_path, labels='inferred', label_mode='categorical', \n",
        "                                                                  image_size=IM_SIZE, batch_size=32, shuffle=True, \n",
        "                                                                  validation_split=0.05, subset='validation', seed=seed)\n",
        "                                                                  \n",
        "rgb_test_path = \"C:/Users/awnya/Documents/Projects/RGBD Object Classification/RGB_dataset/test\"\n",
        "rgb_dataset_test = image_dataset_from_directory(rgb_test_path, labels='inferred', label_mode='categorical', \n",
        "                                                                  image_size=IM_SIZE, batch_size=32, shuffle=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4608 files belonging to 2 classes.\n",
            "Using 4378 files for training.\n",
            "Found 4608 files belonging to 2 classes.\n",
            "Using 230 files for validation.\n",
            "Found 1316 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple', 'banana']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "rgb_dataset_train.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "rgb_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 76s 12s/step - loss: 1.7522 - accuracy: 0.4878 - val_loss: 0.1144 - val_accuracy: 0.9609\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x220a5bcf518>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "rgb_model.fit(rgb_dataset_train, epochs=1, steps_per_epoch=5, validation_data=rgb_dataset_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "rgb_model.save_weights('weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 163s 4s/step - loss: 0.2357 - accuracy: 0.9157\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23565857112407684, 0.9156534671783447]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "rgb_model.evaluate(rgb_dataset_test)"
      ]
    }
  ]
}